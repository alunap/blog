[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Dealing with multi-class ML data\n\n\n\n\n\n\n\ncode\n\n\njulia\n\n\nml\n\n\ndata cleaning\n\n\ndata engineering\n\n\n\n\n\n\n\n\n\n\n\nSep 20, 2023\n\n\nDr Alun ap Rhisiart\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To The Wanderings Blog\n\n\n\n\n\n\n\nnew\n\n\n\n\n\n\n\n\n\n\n\nSep 17, 2023\n\n\nAlun ap Rhisiart\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Dealing with multi-class ML data",
    "section": "",
    "text": "For the purpose of child safeguarding, one thing I do is build NLP models that classify texts according to certain risk categories (eating disorders, suicide ideation, drugs, gambling, etc). For this purpose we needed to take text data that was captured (either written or viewed) and have an external annotation company label the texts for us. This was meant to be via Amazon Ground Truth, but because of some issues with the setup at the time we ended up dealing directly with the annotation company."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To The Wanderings Blog",
    "section": "",
    "text": "In my work, I use a variety of languages, principally SQL, Julia, Python, and R, and a variety of tools, such as Databricks, dbt, Prefect, Jupyter Lab, VS Code, RStudio, and more. There are always problems to solve, and here I’ll blog about some problems that were overcome, and some techniques to help us on our way. Maybe it will be of use to someone apart from myself."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "A meandering through data science and engineering adventures in Julia, Python, and R."
  },
  {
    "objectID": "posts/post-with-code/index.html#eyballing-the-data",
    "href": "posts/post-with-code/index.html#eyballing-the-data",
    "title": "Dealing with multi-class ML data",
    "section": "Eyballing the data",
    "text": "Eyballing the data\nThe label column was in the form of a string such as:\n[‘suicide’, ‘mental health’, ‘self-harm’]\nThis may look like an array of strings, but it was actually just a single string starting and ending with a square bracket. This would need to be parsed.\nThe first thing I find is that there are some inconsistencies in the labels field. For example:\n\ntypos such as ‘suicide’ vs ‘sucide’\ndifferences in case (‘weapon’ vs ‘weapons’)\nsome were plural, some were singular (‘weapon’ vs ‘weapons’)\nand we had both ‘self-harm’ and ‘self harm’ and ‘Games’ vs ‘Games.’ (a tricky one this because of the next issue)\nsometimes the separator was a period instead of a comma\nThe column names were not valid Julia dataframe column identifiers, for example ‘Final Label’.\nIn some cases there was an extra bracket in the list of labels, [‘label1’, ‘label2’ ‘label3’]] (this one was rare, so it was cleaned up in Excel and rexported to csv).\nSome of the labels had missing quotes, [‘label1’, ‘label2, label3’]"
  },
  {
    "objectID": "posts/post-with-code/index.html#introduction",
    "href": "posts/post-with-code/index.html#introduction",
    "title": "Dealing with multi-class ML data",
    "section": "",
    "text": "For the purpose of child safeguarding, one thing I do is build NLP models that classify texts according to certain risk categories (eating disorders, suicide ideation, drugs, gambling, etc). For this purpose we needed to take text data that was captured (either written or viewed) and have an external annotation company label the texts for us. This was meant to be via Amazon Ground Truth, but because of some issues with the setup at the time we ended up dealing directly with the annotation company."
  },
  {
    "objectID": "posts/post-with-code/index.html#the-data",
    "href": "posts/post-with-code/index.html#the-data",
    "title": "Dealing with multi-class ML data",
    "section": "The data",
    "text": "The data\nThe data came back in two spreadsheets. Each sentence had been checked by three separate annotators, and the ones where everybody agreed on the classifications went into a ‘consensus’ spreadsheet; where there was disagreement it appears a forth person (a supervisor) checked and decided which classifications should stand, and this data came in a ‘nonconsensus’ spreadsheet. Note the plural: I asked them to assign all applicable classifications to a sentence, not just the primary one. A sentence could be about both ‘mental health’ and ‘drug use’, for example. I did a spot check of a good number of the 40,000 sentences, and was very pleased with the accuracy of the result (which is to say, I agreed with their decisions). Nonetheless, there were some data engineering challenges to get the data into a suitable format (there always are!)."
  },
  {
    "objectID": "posts/cleaning-ml-data/index.html",
    "href": "posts/cleaning-ml-data/index.html",
    "title": "Dealing with multi-class ML data",
    "section": "",
    "text": "For the purpose of child safeguarding, one thing I do is build NLP models that classify texts according to certain risk categories (eating disorders, suicide ideation, drugs, gambling, etc). For this purpose I needed to take text data that was captured (either written or viewed) and have an external annotation company label the texts for me. This was meant to be via Amazon Ground Truth, but because of some issues with the setup at the time I ended up dealing directly with the annotation company."
  },
  {
    "objectID": "posts/cleaning-ml-data/index.html#introduction",
    "href": "posts/cleaning-ml-data/index.html#introduction",
    "title": "Dealing with multi-class ML data",
    "section": "",
    "text": "For the purpose of child safeguarding, one thing I do is build NLP models that classify texts according to certain risk categories (eating disorders, suicide ideation, drugs, gambling, etc). For this purpose I needed to take text data that was captured (either written or viewed) and have an external annotation company label the texts for me. This was meant to be via Amazon Ground Truth, but because of some issues with the setup at the time I ended up dealing directly with the annotation company."
  },
  {
    "objectID": "posts/cleaning-ml-data/index.html#the-data",
    "href": "posts/cleaning-ml-data/index.html#the-data",
    "title": "Dealing with multi-class ML data",
    "section": "The data",
    "text": "The data\nThe data came back in two spreadsheets. Each sentence had been checked by three separate annotators, and the ones where everybody agreed on the classifications went into a ‘consensus’ spreadsheet; where there was disagreement it appears a forth person (a supervisor) checked and decided which classifications should stand, and this data came in a ‘noconsensus’ spreadsheet. Note the plural: I asked them to assign all applicable classifications to a text, not just the primary one. A sentence could be about both ‘mental health’ and ‘drug use’, for example. I did a spot check of a good number of the 40,000 sentences, and was very pleased with the accuracy of the result (which is to say, I agreed with their decisions). Nonetheless, there were some data engineering challenges to get the data into a suitable format (there always are!)."
  },
  {
    "objectID": "posts/cleaning-ml-data/index.html#eyballing-the-data",
    "href": "posts/cleaning-ml-data/index.html#eyballing-the-data",
    "title": "Dealing with multi-class ML data",
    "section": "Eyballing the data",
    "text": "Eyballing the data\nThe label column was in the form of a string such as:\n[‘suicide’, ‘mental health’, ‘self-harm’]\nThis may look like an array of strings, but it was actually just a single string starting and ending with a square bracket. This would need to be parsed.\nThe first thing I find is that there are some inconsistencies in the labels field. For example:\n\ntypos such as ‘suicide’ vs ‘sucide’\ndifferences in case (‘Weapon’ vs ‘weapon’)\nsome were plural, some were singular (‘weapon’ vs ‘weapons’)\nand we had both ‘self-harm’ and ‘self harm’ and ‘Games’ vs ‘Games.’ (a tricky one this because of the next issue)\nsometimes the separator was a period instead of a comma\nThe column names were not valid Julia dataframe column identifiers, for example ‘Final Label’.\nIn some cases there was an extra bracket in the list of labels, [‘label1’, ‘label2’ ‘label3’]] (this one was rare, so it was cleaned up in Excel and rexported to csv).\nSome of the labels had missing quotes, [‘label1’, ‘label2, label3’]"
  },
  {
    "objectID": "posts/cleaning-ml-data/index.html#cleaning-the-data",
    "href": "posts/cleaning-ml-data/index.html#cleaning-the-data",
    "title": "Dealing with multi-class ML data",
    "section": "Cleaning the data",
    "text": "Cleaning the data\nFirst load it, dealing with the column names in the process.\n```{julia}\nusing DrWatson\nquickactivate(@__DIR__)\nusing CSV, StatsBase, DataFramesMeta, Parquet, Random, JSONTables\n\nconsensus = CSV.read(datadir(\"import\", \"consensus.csv\"), header=1, ignoreemptyrows=true, normalizenames=true, DataFrame)\nnonconsensus = CSV.read(datadir(\"import\", \"noconsensus.csv\"), header=1, ignoreemptyrows=true, normalizenames=true, DataFrame)\n\ndf = vcat(consensus, nonconsensus)\n```"
  },
  {
    "objectID": "posts/cleaning-ml-data/index.html#loading-and-separating-the-data",
    "href": "posts/cleaning-ml-data/index.html#loading-and-separating-the-data",
    "title": "Dealing with multi-class ML data",
    "section": "Loading and separating the data",
    "text": "Loading and separating the data\nFirst load it, dealing with the column names in the process. I used DrWatson to set up by Julia projects.\n```{julia}\nusing DrWatson\nquickactivate(@__DIR__)\nusing CSV, StatsBase, DataFramesMeta, Parquet, Random, JSONTables\n\nconsensus = CSV.read(datadir(\"import\", \"consensus.csv\"), header=1, ignoreemptyrows=true, normalizenames=true, DataFrame)\nnonconsensus = CSV.read(datadir(\"import\", \"noconsensus.csv\"), header=1, ignoreemptyrows=true, normalizenames=true, DataFrame)\n\ndf = vcat(consensus, nonconsensus)\n```"
  },
  {
    "objectID": "posts/cleaning-ml-data/index.html#adding-back-the-labels",
    "href": "posts/cleaning-ml-data/index.html#adding-back-the-labels",
    "title": "Dealing with multi-class ML data",
    "section": "Adding back the labels",
    "text": "Adding back the labels\nWe have the texts separated, now we want to add an integer label rather than the text label. While we are at it, drop any unnecessary columns and rename ‘data_x’ to ‘text’.\n```{julia}\nfunction setlabels(df, label)\n  data = select(df, :id, :data_x =&gt; :text)\n  data[!, :label] .= label\n  return data\nend\n\nnegatives = setlabels(negatives, 0)\nself_harm = setlabels(self_harm, 4)\n# and so on for all of them\nall_texts = vcat(negatives, self_harm, ) # etc for all the classes\n```"
  },
  {
    "objectID": "posts/cleaning-ml-data/index.html#write-out-the-single-classification-data",
    "href": "posts/cleaning-ml-data/index.html#write-out-the-single-classification-data",
    "title": "Dealing with multi-class ML data",
    "section": "Write out the single-classification data",
    "text": "Write out the single-classification data\nNow we can export, and I do this both for CSV and parquet.\n```{julia}\nCSV.write(datadir(\"exp_pro\", \"all_texts.csv), all_texts, header=true, append=false)\nwrite_parquet(datadir(\"exp_pro\", \"all_texts.parquet\"), all_texts)\n```"
  },
  {
    "objectID": "posts/cleaning-ml-data/index.html#build-the-model-and-examine",
    "href": "posts/cleaning-ml-data/index.html#build-the-model-and-examine",
    "title": "Dealing with multi-class ML data",
    "section": "Build the model and examine",
    "text": "Build the model and examine\nThe model (or actually a number of scikit-learn models) were built in python, which I could use PythonCall for, but let’s skip that part. This is about handling data, not building models. What I found is that the accuracy was very poor for self-harm, because there were far fewer instances in the dataset. I decided to remove this classification for now, and build a model without it. This is straightforward. In addition, there was a large imbalance between negatives and all the positive classifications, which mean that the balanced accuracy score was significantly lower than the overall accuracy score. I decided to sample down the negatives to bring the numbers roughly in line with the others.\n```{julia}\nnegs = negatives[sample(1:nrow(negatives), 5_700, replace=false), :]\nreduced_texts = vcat(negs, self_harm, ) # etc\n```\nAnd write out the files once more."
  },
  {
    "objectID": "posts/cleaning-ml-data/index.html#split-into-trainvalidationtest-sets",
    "href": "posts/cleaning-ml-data/index.html#split-into-trainvalidationtest-sets",
    "title": "Dealing with multi-class ML data",
    "section": "Split into train/validation/test sets",
    "text": "Split into train/validation/test sets\nThis worked well for scikit-learn models. I also wanted to build a Distilbert model using hugging-face transformers. Scikit-learn has a nice feature to split test-train, but hugging face apparently does not, so I needed to split beforehand. In addition, I split the training set into a train/validation set, again something that scikit-learn does for us.\n```{julia}\nshuffled_df = Dataframe(shuffle(eachrow(reduced_texts)))\ntrain_size = Int(round(nrow(shuffled_df) * 0.8))\nmh_train = shuffled_df[1:train_size, :]\nmh_test = shuffled_df[train_size + 1:end, :]\n\nvalid_size = Int(round(nrow(mh_train) * 0.85))\nvalid_df = mh_train[valid_size + 1:end, :]\nmh_train = mh_train[1:valid_size, :]\n```\nThese dataframes were then written out, loaded onto Databricks, and a Distilbert model built."
  },
  {
    "objectID": "posts/cleaning-ml-data/index.html#single-label-classifier",
    "href": "posts/cleaning-ml-data/index.html#single-label-classifier",
    "title": "Dealing with multi-class ML data",
    "section": "Single label classifier",
    "text": "Single label classifier\nNow we have have it all in one datafame, the next step is to extract the texts that are associated with a classification. In this case, only 7% of the texts were associoted with more than one classification, and we are not sure whether a multi-class model is needed at all. This is actually quite simple in Julia:\n```{julia}\nnegatives = filter(rows -&gt; occursin.(\"negative\", lowercase(rows.Final_Label)), df)\nself_harm = filter(rows -&gt; occursin.(\"self\", lowercase(rows.Final_Label)), df)\n```\nand so on for all the categories. Here we don’t have to worry about issues like singular or plural, or what separators are used. The final list is going to be a bit more than the original count, since there are some texts that belong to more than one classification.\n```{julia}\ntotal = nrow(negatives) + nrow(self_harm) # +...etc\n```\n\nAdding back the labels\nWe have the texts separated, now we want to add an integer label rather than the text label. While we are at it, drop any unnecessary columns and rename ‘data_x’ to ‘text’.\n```{julia}\nfunction setlabels(df, label)\n  data = select(df, :id, :data_x =&gt; :text)\n  data[!, :label] .= label\n  return data\nend\n\nnegatives = setlabels(negatives, 0)\nself_harm = setlabels(self_harm, 4)\n# and so on for all of them\nall_texts = vcat(negatives, self_harm, ) # etc for all the classes\n```\n\n\nWrite out the single-classification data\nNow we can export, and I do this both for CSV and parquet.\n```{julia}\nCSV.write(datadir(\"exp_pro\", \"all_texts.csv\"), all_texts, header=true, append=false)\nwrite_parquet(datadir(\"exp_pro\", \"all_texts.parquet\"), all_texts)\n```\n\n\nBuild the model and examine\nThe model (or actually a number of scikit-learn models) were built in python, which I could use PythonCall for, but let’s skip that part. This is about handling data, not building models. What I found is that the accuracy was very poor for self-harm, because there were far fewer instances in the dataset. I decided to remove this classification for now, and build a model without it. This is straightforward. In addition, there was a large imbalance between negatives and all the positive classifications, which means that the balanced accuracy score was significantly lower than the overall accuracy score. I decided to downsample the negatives to bring the numbers roughly in line with the others.\n```{julia}\nnegs = negatives[sample(1:nrow(negatives), 5_700, replace=false), :]\nreduced_texts = vcat(negs, eating_disorders, ) # etc\n```\nAnd write out the files once more.\n\n\nSplit into train/validation/test sets\nThis worked well for scikit-learn models. I also wanted to build a Distilbert model using hugging-face transformers. Scikit-learn has a nice feature to split test-train, but hugging face apparently does not, so I needed to split beforehand. In addition, I split the training set into a train/validation set, again something that scikit-learn does for us.\n```{julia}\nshuffled_df = Dataframe(shuffle(eachrow(reduced_texts)))\ntrain_size = Int(round(nrow(shuffled_df) * 0.8))\nmh_train = shuffled_df[1:train_size, :]\nmh_test = shuffled_df[train_size + 1:end, :]\n\nvalid_size = Int(round(nrow(mh_train) * 0.85))\nvalid_df = mh_train[valid_size + 1:end, :]\nmh_train = mh_train[1:valid_size, :]\n```\nThese dataframes were then written out, loaded onto Databricks, and a Distilbert model built."
  },
  {
    "objectID": "posts/cleaning-ml-data/index.html#multi-label-classifier",
    "href": "posts/cleaning-ml-data/index.html#multi-label-classifier",
    "title": "Dealing with multi-class ML data",
    "section": "Multi-label classifier",
    "text": "Multi-label classifier\nWe have data that has multiple labels associated with the same text where appropriate. There were nothing like as many as I was expecting, but worth seeing whether building a one-vs-rest model that took account of all the labels would do a good job. This is not so straightforward, however, because now we have to parse each part of the label, taking account of inconsistencies like missing quotes, periods inside the label, etc, and convert the strings to integers.\nThe first thing is to take account of the inconsistent names (‘weapons’ vs ‘weapon’, ‘self-harm’ vs ‘self harm’, etc). I did this by making a dictionary mapping the strings to integers, and simply added extra items for variants, mapping to the same integers.\n```{julia}\npolicy_names = Dict(\n  \"negative\" == 0,\n  \"eating disorder\" == 1,\n  \"suicide\" == 2,\n  \"sucide\" == 2 # and so on for all the classifications and all the variants\n  ) \n```\nGiven a string such as “[‘class1’, ‘class3’…]” We can extract each policy name, strip extra characters, look up the label numbers, and return an array of integers. Optionally exclude any policies we don’t want at this time (eg self-harm).\n```{julia}\nfunction findepolicy(str, exclusions=[])\n  isdelim(char) = char == '.' || char == ','\n  splits = split(str, isdelim)\n  items = map(x -&gt; strip(x, [ '[', ']', ' ', ''', '\"', '.' ]), splits)\n  result = map(item -&gt; policy_names(lowercase(item)), items)\n  return setdiff!result, exclusions)\nend\n\ncoded_df = @chain df begin\n  @rtransform(:labels = findpolicy(:Final_Label))\n  select(:id, :data_x =&gt; :text, :labels)\nend\n```\nTo load it into pandas. I converted to JSON. W ecan use JSONTables for this\n```{julia}\njsonstring = arraytable(coded_df)\nopen(datadir(\"exp_pro\", \"multiclass_captures_texts.json\"), \"w\") do f\n  write(f, jsonstring)\nend\n```\nFinally, I made a reduced version of the multiclass to exclude self-harm. One addition here is that we need to guard against cases where self-harm is the only label for a string.\n```{julia}\nreduced_df = @chain df begin\n  @rtransform(:labels = findpolicy(:Final_Label, [4]))\n  select(:id, :data_x =&gt; :text, :labels)\n  @rsubset(:labels != [])\nend\n```\nAnd that’s it! As always, I was impressed by the power of Julia dataframes and macros to do a great deal in very few lines, and run fast. I practice, I use whatever language seems to best suit the task at hand, whether that be Python, R, Julia, or SQL. My favourite is Julia, but that isn’t available on Databricks. For statistics (including Bayesian models) I generally prefer R, and I also like the geocomputing setup on R. All three are good at graphics, although some have facilities that others don’t, but most of the time I will use ggplot2 in R or Makie in Julia, but Seaborn is also fine, and I like Folium. Python still leads the pack on ML libraries, with both scikit-learn and huggingface being indispensible."
  },
  {
    "objectID": "posts/cleaning-ml-data/index.html#eyeballing-the-data",
    "href": "posts/cleaning-ml-data/index.html#eyeballing-the-data",
    "title": "Dealing with multi-class ML data",
    "section": "Eyeballing the data",
    "text": "Eyeballing the data\nThe data was of of the form\n\n\n\n\n\n\n\n\n\n\n\nid\ndata_x\nlabel1\nlabel2\nlabel3\nFinal Label\n\n\n\n\n1\n“Sample text”\n[‘negative’]\n[‘drugs’]\n[‘drugs’, ‘mental health’]\n[‘drugs’]\n\n\n2\n“another sample”\n[‘drugs’]\n[‘suicide’]\n[‘drugs’]\n[‘drugs’]\n\n\n\nThe label columns were in the form of a string such as:\n[‘suicide’, ‘mental health’, ‘self-harm’]\nThis may look like an array of strings, but it was actually just a single string starting and ending with a square bracket. This would need to be parsed and the strings converted to integer labels.\nThe first thing I found is that there are some inconsistencies in the labels field. For example:\n\ntypos such as ‘suicide’ vs ‘sucide’\ndifferences in case (‘Weapon’ vs ‘weapon’)\nsome were plural, some were singular (‘weapon’ vs ‘weapons’)\nand we had both ‘self-harm’ and ‘self harm’ and ‘Games’ vs ‘Games.’ (a tricky one this because of the next issue)\nsometimes the separator was a period instead of a comma\nThe column names were not valid Julia dataframe column identifiers, for example ‘Final Label’.\nIn some cases there was an extra bracket in the list of labels, [‘label1’, ‘label2’, ‘label3’]] (this one was rare, so it was cleaned up in Excel and rexported to csv).\nSome of the labels had missing quotes, [‘label1’, ‘label2, label3’]"
  }
]