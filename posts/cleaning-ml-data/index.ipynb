{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Dealing with multi-class ML data\"\n",
        "author: \"Alun ap Rhisiart\"\n",
        "date: \"2023-09-20\"\n",
        "categories: [code, julia, ml, data cleaning]\n",
        "---"
      ],
      "id": "a767123e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n",
        "\n",
        "For the purpose of child safeguarding, one thing I do is build NLP models that classify texts according to certain risk categories (eating disorders, suicide ideation, drugs, gambling, etc). For this purpose we needed to take text data that was captured (either written or viewed) and have an external annotation company label the texts for us. This was meant to be via Amazon Ground Truth, but because of some issues with the setup at the time we ended up dealing directly with the annotation company. \n",
        "\n",
        "## The data\n",
        "The data came back in two spreadsheets. Each sentence had been checked by three separate annotators, and the ones where everybody agreed on the classifications went into a 'consensus' spreadsheet; where there was disagreement it appears a forth person (a supervisor) checked and decided which classifications should stand, and this data came in a 'nonconsensus' spreadsheet. Note the plural: I asked them to assign all applicable classifications to a sentence, not just the primary one. A sentence could be about both 'mental health' and 'drug use', for example. I did a spot check of a good number of the 40,000 sentences, and was very pleased with the accuracy of the result (which is to say, I agreed with their decisions). Nonetheless, there were some data engineering challenges to get the data into a suitable format (there always are!).\n",
        "\n",
        "## Eyeballing the data\n",
        "The label column was in the form of a string such as:\n",
        "\n",
        "['suicide', 'mental health', 'self-harm']\n",
        "\n",
        "This may look like an array of strings, but it was actually just a single string starting and ending with a square bracket. This would need to be parsed.\n",
        "\n",
        "The first thing I find is that there are some inconsistencies in the labels field. For example:\n",
        " \n",
        "- typos such as 'suicide' vs 'sucide'\n",
        "- differences in case ('Weapon' vs 'weapon')\n",
        "- some were plural, some were singular ('weapon' vs 'weapons')\n",
        "- and we had both 'self-harm' and 'self harm' and 'Games' vs 'Games.' (a tricky one this because of the next issue)\n",
        "- sometimes the separator was a period instead of a comma\n",
        "- The column names were not valid Julia dataframe column identifiers, for example 'Final Label'.\n",
        "- In some cases there was an extra bracket in the list of labels, ['label1', 'label2' 'label3']] (this one was rare, so it was cleaned up in Excel and rexported to csv).\n",
        "- Some of the labels had missing quotes, ['label1', 'label2, label3']\n",
        "\n",
        "## Loading and separating the data\n",
        "\n",
        "First load it, dealing with the column names in the process.\n",
        "```{{julia}}\n",
        "using DrWatson\n",
        "quickactivate(@__DIR__)\n",
        "using CSV, StatsBase, DataFramesMeta, Parquet, Random, JSONTables\n",
        "\n",
        "consensus = CSV.read(datadir(\"import\", \"consensus.csv\"), header=1, ignoreemptyrows=true, normalizenames=true, DataFrame)\n",
        "nonconsensus = CSV.read(datadir(\"import\", \"noconsensus.csv\"), header=1, ignoreemptyrows=true, normalizenames=true, DataFrame)\n",
        "\n",
        "df = vcat(consensus, nonconsensus)\n",
        "```\n",
        "\n",
        "## Single label classifier\n",
        "Now we have have it all in one datafame, the next step is to extract the texts that are associated with a classification. In this case, only 7% of the texts were associoted with more than one classification, and we are not sure whether a multi-class model is needed at all. This is actually quite simple in Julia:\n",
        "```{{julia}}\n",
        "negatives = filter(rows -> occursin.(\"negative\", lowercase(rows.Final_Label)), df)\n",
        "self_harm = filter(rows -> occursin.(\"self\", lowercase(rows.Final_Label)), df)\n",
        "```\n",
        "and so on for all the categories. Here we don't have to worry about issues like singular or plural, or what separators are used. The final list is going to be a bit more than the original count, since there are some texts that belong to more than one classification.\n",
        "```{{julia}}\n",
        "total = nrow(negatives) + nrow(self_harm) # +...etc\n",
        "```\n",
        "\n",
        "#### Adding back the labels\n",
        "We have the texts separated, now we want to add an integer label rather than the text label. While we are at it, drop any unnecessary columns and rename 'data_x' to 'text'.\n",
        "\n",
        "```{{julia}}\n",
        "function setlabels(df, label)\n",
        "  data = select(df, :id, :data_x => :text)\n",
        "  data[!, :label] .= label\n",
        "  return data\n",
        "end\n",
        "\n",
        "negatives = setlabels(negatives, 0)\n",
        "self_harm = setlabels(self_harm, 4)\n",
        "# and so on for all of them\n",
        "all_texts = vcat(negatives, self_harm, ) # etc for all the classes\n",
        "```\n",
        "#### Write out the single-classification data\n",
        "Now we can export, and I do this both for CSV and parquet.\n",
        "```{{julia}}\n",
        "CSV.write(datadir(\"exp_pro\", \"all_texts.csv\"), all_texts, header=true, append=false)\n",
        "write_parquet(datadir(\"exp_pro\", \"all_texts.parquet\"), all_texts)\n",
        "```\n",
        "#### Build the model and examine\n",
        "The model (or actually a number of scikit-learn models) were built in python, which I could use PythonCall for, but let's skip that part. This is about handling data, not building models.\n",
        "What I found is that the accuracy was very poor for self-harm, because there were far fewer instances in the dataset. I decided to remove this classification for now, and build a model without it. This is straightforward. In addition, there was a large imbalance between negatives and all the positive classifications, which means that the balanced accuracy score was significantly lower than the overall accuracy score. I decided to downsample the negatives to bring the numbers roughly in line with the others.\n",
        "\n",
        "```{{julia}}\n",
        "negs = negatives[sample(1:nrow(negatives), 5_700, replace=false), :]\n",
        "reduced_texts = vcat(negs, eating_disorders, ) # etc\n",
        "```\n",
        "And write out the files once more. \n",
        "\n",
        "#### Split into train/validation/test sets\n",
        "This worked well for scikit-learn models. I also wanted to build a Distilbert model using hugging-face transformers. Scikit-learn has a nice feature to split test-train, but hugging face apparently does not, so I needed to split beforehand. In addition, I split the training set into a train/validation set, again something that scikit-learn does for us.\n",
        "```{{julia}}\n",
        "shuffled_df = Dataframe(shuffle(eachrow(reduced_texts)))\n",
        "train_size = Int(round(nrow(shuffled_df) * 0.8))\n",
        "mh_train = shuffled_df[1:train_size, :]\n",
        "mh_test = shuffled_df[train_size + 1:end, :]\n",
        "\n",
        "valid_size = Int(round(nrow(mh_train) * 0.85))\n",
        "valid_df = mh_train[valid_size + 1:end, :]\n",
        "mh_train = mh_train[1:valid_size, :]\n",
        "```\n",
        "\n",
        "These dataframes were then written out, loaded onto Databricks, and a Distilbert model built.\n",
        "\n",
        "## Multi-label classifier\n",
        "We have data that has multiple labels associated with the same text where appropriate. There were nothing like as many as I was expecting, but worth seeing whether building a one-vs-rest model that took account of all the labels would do a good job. This is not so straightforward, however, because now we have to parse each part of the label, taking account of inconsistencies like missing quotes, periods inside the label, etc, and convert the strings to integers.\n",
        "\n",
        "The first thing is to take account of the inconsistent names ('weapons' vs 'weapon', 'self-harm' vs 'self harm', etc). I did this by making a dictionary mapping the strings to integers, and simply added extra items for variants, mapping to the same integers.\n",
        "```{{julia}}\n",
        "policy_names = Dict(\n",
        "  \"negative\" == 0,\n",
        "  \"eating disorder\" == 1,\n",
        "  \"suicide\" == 2,\n",
        "  \"sucide\" == 2 # and so on for all the classifications and all the variants\n",
        "  ) \n",
        "```\n",
        "Given a string such as \"['class1', 'class3'...]\" We can extract each policy name, strip extra characters, look up the label numbers, and return an array of integers. Optionally exclude any policies we don't want at this time (eg self-harm).\n",
        "\n",
        "```{{julia}}\n",
        "function findepolicy(str, exclusions=[])\n",
        "  isdelim(char) = char == '.' || char == ','\n",
        "  splits = split(str, isdelim)\n",
        "  items = map(x -> strip(x, [ '[', ']', ' ', ''', '\"', '.' ]), splits)\n",
        "  result = map(item -> policy_names(lowercase(item)), items)\n",
        "  return setdiff!result, exclusions)\n",
        "end\n",
        "\n",
        "coded_df = @chain df begin\n",
        "  @rtransform(:labels = findpolicy(:Final_Label))\n",
        "  select(:id, :data_x => :text, :labels)\n",
        "end\n",
        "```\n",
        "To load it into pandas. I converted to JSON. W ecan use JSONTables for this\n",
        "```{{julia}}\n",
        "jsonstring = arraytable(coded_df)\n",
        "open(datadir(\"exp_pro\", \"multiclass_captures_texts.json\"), \"w\") do f\n",
        "  write(f, jsonstring)\n",
        "end\n",
        "```\n",
        "\n",
        "Finally, I made a reduced version of the multiclass to exclude self-harm. One addition here is that we need to guard against cases where self-harm is the only label for a string."
      ],
      "id": "a20f2372"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "reduced_df = @chain df begin\n",
        "  @rtransform(:labels = findpolicy(:Final_Label, [4]))\n",
        "  select(:id, :data_x => :text, :labels)\n",
        "  @rsubset(:labels != [])\n",
        "end"
      ],
      "id": "a8f7a6d0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And that's it! As always, I was impressed by the power of Julia dataframes and macros to do a great deal in very few lines, and run fast."
      ],
      "id": "6172f002"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "julia-1.9",
      "language": "julia",
      "display_name": "Julia 1.9.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}